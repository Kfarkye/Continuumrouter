// useAiRouterChat.ts
import { useState, useEffect, useCallback, useRef, useMemo } from 'react';
import { ChatMessage, StoredFile, FileAttachment, Memory } from '../types';
import { supabase } from '../lib/supabaseClient';
import { captureMemory, retrieveMemories } from '../services/memoryService';

// -----------------------------------------------------------------------------
// Configuration and Constants
// -----------------------------------------------------------------------------

const AI_ROUTER_FUNCTION_URL = `${import.meta.env.VITE_SUPABASE_URL}/functions/v1/ai-chat-router`;

// Helper to safely attempt JSON parsing of responses
const safeJsonParse = async (response: Response): Promise<any> => {
    try {
        const text = await response.text();
        if (!text) return null;
        return JSON.parse(text);
    } catch {
        return null;
    }
};

const DEFAULT_CONFIG = {
Â  IDLE_TIMEOUT_MS: 120_000,
Â  RETRY_COUNT: 3, // Increased for resilience
Â  RETRY_BASE_DELAY_MS: 1000,
Â  RECENT_MESSAGE_LIMIT: 50,
  MEMORY_RETRIEVAL_TIMEOUT_MS: 5000, // NEW: Timeout for memory service
};

// -----------------------------------------------------------------------------
// Types and Interfaces
// -----------------------------------------------------------------------------

export interface ChatError {
Â  code: string;
Â  message: string;
Â  details?: unknown;
}

export interface UseAiRouterChatArgs {
Â  sessionId: string | null;
Â  accessToken: string | null;
Â  userId: string | null;
Â  projectId?: string | null;
Â  files: StoredFile[];
Â  onActionRequest: (action: string, args: any) => Promise<void>;
Â  selectedModel?: 'auto' | 'claude' | 'gemini' | 'gpt';
Â  config?: Partial<typeof DEFAULT_CONFIG>;
}

// ENHANCEMENT: Added 'edit' reason
type AbortReason = 'user' | 'timeout' | 'internal' | 'edit';

// Discriminated Union for Stream Chunks
interface ProgressChunk { type: 'progress'; progress: number; step: string; }
interface ModelSwitchChunk { type: 'model_switch'; model: string; content?: string; metadata?: Record<string, unknown>; }
interface TextChunk { type: 'text'; content: string; }
interface ActionRequestChunk { type: 'action_request'; action: string; content: unknown; }
interface MetadataChunk { type: 'metadata'; content: Record<string, unknown>; }
interface ErrorChunk { type: 'error'; content: unknown; }
interface InfoChunk { type: 'warning' | 'success'; content: unknown; }
interface DoneChunk { type: 'done'; }

type RouterChunk =
Â  | ProgressChunk | ModelSwitchChunk | TextChunk | ActionRequestChunk
Â  | MetadataChunk | ErrorChunk | InfoChunk | DoneChunk;

// -----------------------------------------------------------------------------
// Utility Functions and Hooks
// -----------------------------------------------------------------------------

function useLatestRef<T>(value: T) {
Â  Â  const ref = useRef(value);
Â  Â  useEffect(() => { ref.current = value; }, [value]);
Â  Â  return ref;
}

class TextAccumulator {
Â  Â  private parts: string[] = [];
Â  Â  private cache = '';
Â  Â  private consumed = 0;
Â  Â  append(s: string) { if (!s) return; this.parts.push(s); }
Â  Â  value(): string {
Â  Â  Â  Â  if (this.consumed < this.parts.length) {
Â  Â  Â  Â  Â  Â  const newSeg = this.parts.slice(this.consumed).join('');
Â  Â  Â  Â  Â  Â  this.cache += newSeg;
Â  Â  Â  Â  Â  Â  this.consumed = this.parts.length;
Â  Â  Â  Â  }
Â  Â  Â  Â  return this.cache;
Â  Â  }
Â  Â  clear() { this.parts = []; this.cache = ''; this.consumed = 0; }
}

function calculateRetryAfter(headerValue: string): number {
Â  Â  const seconds = parseInt(headerValue, 10);
Â  Â  if (!isNaN(seconds)) return Math.max(0, seconds * 1000);
Â  Â  const date = new Date(headerValue);
Â  Â  if (!isNaN(date.getTime())) return Math.max(0, date.getTime() - Date.now());
Â  Â  return 0;
}

// ENHANCEMENT: Improved retry logic with better jitter and error handling
async function fetchWithRetry(url: string, options: RequestInit, retries: number, delay: number): Promise<Response> {
    let lastErr: unknown;
    let lastResponse: Response | null = null;

Â  Â  for (let i = 0; i <= retries; i++) {
Â  Â  Â  Â  try {
Â  Â  Â  Â  Â  Â  const response = await fetch(url, options);
            lastResponse = response;

Â  Â  Â  Â  Â  Â  if (!response.ok) {
Â  Â  Â  Â  Â  Â  Â  Â  const status = response.status;
Â  Â  Â  Â  Â  Â  Â  Â  if ((status >= 500 || status === 429) && i < retries) {
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  const retryAfterHeader = response.headers.get('Retry-After');
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  let backoffMs = status === 429 && retryAfterHeader ? calculateRetryAfter(retryAfterHeader) : delay * Math.pow(2, i);

                    // Add Jitter (+/- 25%) to prevent thundering herd problem
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  const jitter = (Math.random() * 0.5 - 0.25) * backoffMs;
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  const waitTime = Math.min(60000, Math.max(100, backoffMs + jitter)); // Cap at 1 min

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  console.warn(`Retrying request (Attempt ${i+1}) due to status ${status}. Waiting ${waitTime.toFixed(0)}ms.`);
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  await new Promise((r) => setTimeout(r, waitTime));
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  continue;
Â  Â  Â  Â  Â  Â  Â  Â  }
Â  Â  Â  Â  Â  Â  }
Â  Â  Â  Â  Â  Â  return response;
Â  Â  Â  Â  } catch (e) {
Â  Â  Â  Â  Â  Â  lastErr = e;
Â  Â  Â  Â  Â  Â  if (e instanceof DOMException && e.name === 'AbortError') throw e;
Â  Â  Â  Â  Â  Â  if (i < retries) {
Â  Â  Â  Â  Â  Â  Â  Â  const backoffMs = delay * Math.pow(2, i);
                const jitter = (Math.random() * 0.5 - 0.25) * backoffMs;
                const waitTime = Math.min(60000, Math.max(100, backoffMs + jitter));

Â  Â  Â  Â  Â  Â  Â  Â  console.warn(`Retrying request (Attempt ${i+1}) due to network error. Waiting ${waitTime.toFixed(0)}ms. Error: ${(e as Error)?.message}`);
Â  Â  Â  Â  Â  Â  Â  Â  await new Promise((r) => setTimeout(r, waitTime));
Â  Â  Â  Â  Â  Â  Â  Â  continue;
Â  Â  Â  Â  Â  Â  }
Â  Â  Â  Â  }
Â  Â  }

    if (lastErr) {
Â  Â      throw lastErr instanceof Error ? lastErr : new Error(`All retry attempts failed due to network issues. Last error: ${String(lastErr)}`);
    }

    // Fallback if loop finishes unexpectedly (e.g., retries exhausted on failed HTTP responses)
    if (lastResponse) return lastResponse;
    throw new Error('fetchWithRetry failed unexpectedly.');
}

function parseLineToChunk(line: string): RouterChunk | null {
Â  Â  // ... (Implementation remains the same as it is robust)
}


// -----------------------------------------------------------------------------
// Main Hook (Simplified)
// -----------------------------------------------------------------------------

export const useAiRouterChat = ({
Â  sessionId,
Â  accessToken,
Â  userId,
Â  projectId,
Â  files,
Â  onActionRequest,
Â  selectedModel = 'auto',
Â  config: customConfig,
}: UseAiRouterChatArgs) => {
Â  const CONFIG = useMemo(() => ({ ...DEFAULT_CONFIG, ...customConfig }), [customConfig]);

Â  // Core state (Pagination state removed)
Â  const [messages, setMessages] = useState<ChatMessage[]>([]);
Â  const [isSending, setIsSending] = useState(false);
Â  const [isLoadingHistory, setIsLoadingHistory] = useState(false);

Â  const [currentProgress, setCurrentProgress] = useState(0);
Â  const [currentStep, setCurrentStep] = useState('');
Â  const [error, setError] = useState<ChatError | null>(null);

Â  // Stable refs and control flow
Â  const abortControllerRef = useRef<AbortController | null>(null);
Â  const abortReasonRef = useRef<AbortReason | null>(null);
Â  const mountedRef = useRef(true);
Â  const onActionRequestRef = useLatestRef(onActionRequest);
Â  const isSendingRef = useLatestRef(isSending);

Â  // RAF-Throttled Streaming Accumulators
Â  const assistantTextAccRef = useRef(new TextAccumulator());
Â  const assistantMessageIdRef = useRef<string | null>(null);
Â  const assistantMetadataRef = useRef<Record<string, unknown>>({});
Â  const assistantStatusRef = useRef<'streaming' | 'complete' | 'error'>('streaming');
Â  const assistantProgressRef = useRef<number>(0);
Â  const assistantStepRef = useRef<string>('');
Â  const rafIdRef = useRef<number | null>(null);

Â  // O(1) file lookup map
Â  const filesById = useMemo(() => {
Â  Â  const map = new Map<string, StoredFile>();
Â  Â  const fileList = Array.isArray(files) ? files : [];
Â  Â  for (const f of fileList) {
Â  Â  Â  Â  if (f && f.id) {
Â  Â  Â  Â  Â  Â map.set(f.id, f);
Â  Â  Â  Â  }
Â  Â  }
Â  Â  return map;
Â  }, [files]);

Â  // Cleanup on unmount
Â  useEffect(() => {
Â  Â  mountedRef.current = true;
Â  Â  return () => {
Â  Â  Â  mountedRef.current = false;
Â  Â  Â  if (isSendingRef.current) {
Â  Â  Â  Â  abortReasonRef.current = 'internal';
Â  Â  Â  }
Â  Â  Â  abortControllerRef.current?.abort();
Â  Â  Â  if (rafIdRef.current != null) cancelAnimationFrame(rafIdRef.current);
Â  Â  };
Â  }, [isSendingRef]);

Â  // Load history when sessionId changes (Simplified: Load Recent Subset)
Â  useEffect(() => {
    // ... (Implementation remains the same as the robust version provided in the prompt)
Â  }, [sessionId, isSendingRef, CONFIG.RECENT_MESSAGE_LIMIT]);

Â  // scheduleAssistantCommit (RAF batching)
Â  const scheduleAssistantCommit = useCallback(() => {
Â  Â  // ... (Implementation remains the same as the robust version provided in the prompt)
Â  }, []);

Â  // cancelStream
Â  const cancelStream = useCallback((reason: AbortReason = 'user') => {
Â  Â  if (abortControllerRef.current && isSendingRef.current) {
Â  Â  Â  abortReasonRef.current = reason;
Â  Â  Â  abortControllerRef.current.abort();
      // Provide feedback unless it's an edit (which should be seamless)
Â  Â  Â  if (reason === 'user') {
        setCurrentStep('Cancelling...');
      } else if (reason !== 'edit') {
Â  Â  Â    setCurrentStep('Stopping...');
      }
Â  Â  }
Â  }, [isSendingRef]);

Â  // sendMessage
Â  const sendMessage = useCallback(
Â  Â  async (content: string, attachedFileIds: string[], imageIds: string[] = []) => {
Â  Â  Â  // 1. Guards and Initialization
      // ... (Guards and Initialization remain the same)

Â  Â  Â  // 2. Prepare Data and Optimistic UI Update
      // ... (Preparation and Optimistic Update remain the same)

Â  Â  Â  // 3. Network Request and Stream Processing
Â  Â  Â  try {
Â  Â  Â  Â  abortControllerRef.current?.abort();
Â  Â  Â  Â  abortControllerRef.current = new AbortController();
Â  Â  Â  Â  const providerHint = selectedModel === 'auto' ? undefined : (selectedModel === 'gpt' ? 'openai' : selectedModel);

Â  Â  Â  Â  // ENHANCEMENT: Robust Memory Retrieval with Timeout
Â  Â  Â  Â  let relevantMemories: Memory[] = [];
Â  Â  Â  Â  if (accessToken && projectId) {
Â  Â  Â  Â  Â  try {
            assistantStepRef.current = 'Retrieving context...';
            scheduleAssistantCommit();

Â  Â  Â  Â  Â  Â  const memoryPromise = retrieveMemories(content, accessToken, projectId);
            // Create a timeout promise
            const timeoutPromise = new Promise<Memory[]>((_, reject) =>
                setTimeout(() => reject(new Error("Memory retrieval timed out")), CONFIG.MEMORY_RETRIEVAL_TIMEOUT_MS)
            );

            // Race the memory retrieval against the timeout
            relevantMemories = await Promise.race([memoryPromise, timeoutPromise]);

Â  Â  Â  Â  Â  Â  if (relevantMemories.length > 0) {
Â  Â  Â  Â  Â  Â  Â  console.log(`ðŸ“š Retrieved ${relevantMemories.length} relevant memories for context`);
Â  Â  Â  Â  Â  Â  }
Â  Â  Â  Â  Â  } catch (err) {
            // Memory retrieval failure/timeout should not block the chat request.
Â  Â  Â  Â  Â  Â  console.warn('Memory retrieval failed or timed out, continuing without memories:', err);
Â  Â  Â  Â  Â  }
Â  Â  Â  Â  }

        assistantStepRef.current = 'Contacting AI Router...';
        scheduleAssistantCommit();

Â  Â  Â  Â  const response = await fetchWithRetry(
Â  Â  Â  Â  Â  AI_ROUTER_FUNCTION_URL,
Â  Â  Â  Â  Â  {
            // ... (Request options remain the same)
Â  Â  Â  Â  Â  },
Â  Â  Â  Â  Â  CONFIG.RETRY_COUNT,
Â  Â  Â  Â  Â  CONFIG.RETRY_BASE_DELAY_MS
Â  Â  Â  Â  );

        // ENHANCEMENT: Improved error response parsing
Â  Â  Â  Â  if (!response.ok) {
Â  Â  Â  Â  Â  const errorData = await safeJsonParse(response);
          const specificErrorMessage = errorData?.error || errorData?.message;

          let errorMessage = `HTTP ${response.status}: ${response.statusText}`;
          if (specificErrorMessage) {
              errorMessage = `${specificErrorMessage} (Status: ${response.status})`;
          }
Â  Â  Â  Â  Â  throw new Error(errorMessage);
Â  Â  Â  Â  }

Â  Â  Â  Â  if (!response.body) throw new Error('No response body received.');

        // ... (Stream Processing Loop remains the same)

Â  Â  Â  Â  // 4. Finalization (Success Path)
        // ... (Final state updates and Memory Capture remain the same)

Â  Â  Â  } catch (e: any) {
Â  Â  Â  Â  // 4. Finalization (Error/Abort Path)
Â  Â  Â  Â  setIsSending(false);

Â  Â  Â  Â  const isAbort = e instanceof DOMException && e.name === 'AbortError';
Â  Â  Â  Â  const abortReason = abortReasonRef.current;

        // ENHANCEMENT: Handle 'edit' abort specifically
        if (isAbort && abortReason === 'edit') {
            // If aborted due to edit, we don't update the UI with cancellation status,
            // as a new message is immediately incoming.
            return;
        }

Â  Â  Â  Â  let errorMessage: string;
Â  Â  Â  Â  let errorCode: string;
Â  Â  Â  Â  let displayStatus: string;

Â  Â  Â  Â  // Determine error message
        // ... (Error determination logic remains the same)

Â  Â  Â  Â  // Update UI via RAF
Â  Â  Â  Â  const suffix = isAbort ? `\n\n(${displayStatus})` : `\n\nâŒ Error: ${errorMessage}`;
Â  Â  Â  Â  assistantTextAccRef.current.append(suffix);
Â  Â  Â  Â  assistantStatusRef.current = isAbort ? 'complete' : 'error';
Â  Â  Â  Â  assistantProgressRef.current = isAbort ? 100 : assistantProgressRef.current;
Â  Â  Â  Â  assistantStepRef.current = displayStatus;
Â  Â  Â  Â  scheduleAssistantCommit();

        // Set global error state unless it was an internal or edit abort
Â  Â  Â  Â  if (!(isAbort && (abortReason === 'internal' || abortReason === 'edit'))) {
Â  Â  Â  Â  Â  Â  setError({ code: errorCode, message: errorMessage, details: e?.toString() });
Â  Â  Â  Â  }
Â  Â  Â  }
Â  Â  },
    // Updated dependency array
Â  Â  [sessionId, accessToken, userId, projectId, filesById, selectedModel, CONFIG, isSendingRef, scheduleAssistantCommit, cancelStream, onActionRequestRef]
Â  );


Â  // regenerateMessage (Corrected)
Â  const regenerateMessage = useCallback(() => {
Â  Â  if (isSendingRef.current || messages.length === 0) return;

Â  Â  const lastUserMessage = [...messages].reverse().find(m => m.role === 'user');
Â  Â  if (!lastUserMessage) return;

Â  Â  const lastUserIndex = messages.lastIndexOf(lastUserMessage);
Â  Â  if (lastUserIndex === -1) return;

    // Remove the user message and subsequent assistant messages.
    // sendMessage will add the user message back optimistically.
Â  Â  setMessages(messages.slice(0, lastUserIndex));

Â  Â  const content = lastUserMessage.content;
Â  Â  const attachedFileIds = (lastUserMessage.metadata?.attachedFileIds as string[]) || [];
Â  Â  const imageIds = (lastUserMessage.metadata?.attachedImageIds as string[]) || [];

Â  Â  sendMessage(content, attachedFileIds, imageIds);
Â  }, [messages, sendMessage, isSendingRef]);

  // ENHANCEMENT: editMessage (Allows editing a specific user message and regenerating)
  const editMessage = useCallback((messageId: string, newContent: string) => {
    if (isSendingRef.current) {
        // Cancel the current stream if one is active
        cancelStream('edit');
    }

    const messageIndex = messages.findIndex(m => m.id === messageId);
    if (messageIndex === -1) return;

    const message = messages[messageIndex];

    // Ensure we are editing a user message
    if (message.role !== 'user') return;

    // Remove the edited message and all subsequent messages
    setMessages(messages.slice(0, messageIndex));

    // Resend the new content with original attachments
    const attachedFileIds = (message.metadata?.attachedFileIds as string[]) || [];
    const imageIds = (message.metadata?.attachedImageIds as string[]) || [];

    sendMessage(newContent, attachedFileIds, imageIds);

  }, [messages, sendMessage, isSendingRef, cancelStream]);

Â  // clearMessages
Â  const clearMessages = useCallback(() => {
    // ... (Implementation remains the same)
Â  }, [cancelStream]);

Â  // Return values (Simplified)
Â  return {
Â  Â  messages,
Â  Â  sendMessage,
Â  Â  regenerateMessage,
    editMessage, // NEW
Â  Â  isSending,
Â  Â  isLoadingHistory,
Â  Â  currentProgress,
Â  Â  currentStep,
Â  Â  error,
Â  Â  cancelStream: useCallback(() => cancelStream('user'), [cancelStream]),
Â  Â  clearMessages,
Â  };
};